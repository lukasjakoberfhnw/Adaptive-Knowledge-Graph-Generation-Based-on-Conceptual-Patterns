<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>KGG_7</title>
<meta name="generator" content="Docling HTML Serializer">
<style>
    html {
        background-color: #f5f5f5;
        font-family: Arial, sans-serif;
        line-height: 1.6;
    }
    body {
        max-width: 800px;
        margin: 0 auto;
        padding: 2rem;
        background-color: white;
        box-shadow: 0 0 10px rgba(0,0,0,0.1);
    }
    h1, h2, h3, h4, h5, h6 {
        color: #333;
        margin-top: 1.5em;
        margin-bottom: 0.5em;
    }
    h1 {
        font-size: 2em;
        border-bottom: 1px solid #eee;
        padding-bottom: 0.3em;
    }
    table {
        border-collapse: collapse;
        margin: 1em 0;
        width: 100%;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;
    }
    th {
        background-color: #f2f2f2;
        font-weight: bold;
    }
    figure {
        margin: 1.5em 0;
        text-align: center;
    }
    figcaption {
        color: #666;
        font-style: italic;
        margin-top: 0.5em;
    }
    img {
        max-width: 100%;
        height: auto;
    }
    pre {
        background-color: #f6f8fa;
        border-radius: 3px;
        padding: 1em;
        overflow: auto;
    }
    code {
        font-family: monospace;
        background-color: #f6f8fa;
        padding: 0.2em 0.4em;
        border-radius: 3px;
    }
    pre code {
        background-color: transparent;
        padding: 0;
    }
    .formula {
        text-align: center;
        padding: 0.5em;
        margin: 1em 0;
        background-color: #f9f9f9;
    }
    .formula-not-decoded {
        text-align: center;
        padding: 0.5em;
        margin: 1em 0;
        background: repeating-linear-gradient(
            45deg,
            #f0f0f0,
            #f0f0f0 10px,
            #f9f9f9 10px,
            #f9f9f9 20px
        );
    }
    .page-break {
        page-break-after: always;
        border-top: 1px dashed #ccc;
        margin: 2em 0;
    }
    .key-value-region {
        background-color: #f9f9f9;
        padding: 1em;
        border-radius: 4px;
        margin: 1em 0;
    }
    .key-value-region dt {
        font-weight: bold;
    }
    .key-value-region dd {
        margin-left: 1em;
        margin-bottom: 0.5em;
    }
    .form-container {
        border: 1px solid #ddd;
        padding: 1em;
        border-radius: 4px;
        margin: 1em 0;
    }
    .form-item {
        margin-bottom: 0.5em;
    }
    .image-classification {
        font-size: 0.9em;
        color: #666;
        margin-top: 0.5em;
    }
</style>
</head>
<body>
<div class='page'>
<h2>Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs</h2>
<h2>Fenia Christopoulou , Makoto Miwa 1 2,3 , Sophia Ananiadou 1</h2>
<p>1</p>
<p>National Centre for Text Mining, School of Computer Science, The University of Manchester, United Kingdom 2 Toyota Technological Institute, Nagoya, 468-8511, Japan 3 Artificial Intelligence Research Center (AIRC),</p>
<p>National Institute of Advanced Industrial Science and Technology (AIST), Japan</p>
<p>{ efstathia.christopoulou, sophia.ananiadou @manchester.ac.uk } makoto-miwa@toyota-ti.ac.jp</p>
<h2>Abstract</h2>
<p>Document-level relation extraction is a complex human process that requires logical inference to extract relationships between named entities in text. Existing approaches use graphbased neural models with words as nodes and edges as relations between them, to encode relations across sentences. These models are node-based, i.e., they form pair representations based solely on the two target node representations. However, entity relations can be better expressed through unique edge representations formed as paths between nodes. We thus propose an edge-oriented graph neural model for document-level relation extraction. The model utilises different types of nodes and edges to create a document-level graph. An inference mechanism on the graph edges enables to learn intraand inter-sentence relations using multi-instance learning internally. Experiments on two document-level biomedical datasets for chemical-disease and genedisease associations show the usefulness of the proposed edge-oriented approach. 1</p>
<h2>1 Introduction</h2>
<p>The extraction of relations between named entities in text, known as Relation Extraction (RE), is an important task of Natural Language Processing (NLP). Lately, RE has attracted a lot of attention from the field, in an effort to improve the inference capability of current methods (Zeng et al., 2017; Christopoulou et al., 2018; Luan et al., 2019).</p>
<p>In real-world scenarios, a large amount of relations are expressed across sentences. The task of identifying these relations is named inter-sentence RE. Typically, inter-sentence relations occur in</p>
<p>1 Source code available at https://github.com/ fenchri/edge-oriented-graph</p>
<p>The case of a 40 - year - old patient who underwent an unsuccessful Bilateral optic neuropathy due to combined and treatment . ethambutol and isoniazid is reported . A with an unusual central was found . cadaver kidney transplantation and was treated with scotoma isoniazid ethambutol bilateral retrobulbar neuropathy bitemporal hemianopic</p>
<p>Figure 1: Example of document-level, inter-sentence relations adapted from the CDR dataset (Li et al., 2016a). The solid and dotted lines represent intra- and inter-sentence relations, respectively.</p>
<p>textual snippets with several sentences, such as documents. In these snippets, each entity is usually repeated with the same phrases or aliases, the occurrences of which are often named entity mentions and regarded as instances of the entity. The multiple mentions of the target entities in different sentences can be useful for the identification of inter-sentential relations, as these relations may depend on the interactions of their mentions with other entities in the same document.</p>
<p>As shown in the example of Figure 1, the entities bilateral optic neuropathy , ethambutol and isoniazid have two mentions each, while the entity scotoma has one mention. The relation between the chemical ethambutol and the disease scotoma is clearly inter-sentential. Their association can only be determined if we consider the interactions between the mentions of these entities in different sentences. A mention of bilateral optic neuropathy interacts with a mention of ethambutol in the first sentence. Another mention of the former interacts with the mention of scotoma in the third sentence. This chain of interactions can help us infer that the entity ethambutol has a relation with the entity scotoma .</p>
<p>The most common technique that is currently used to deal with multiple mentions of named entities is Multi-Instance Learning (MIL). Initially, MIL was introduced by Riedel et al. (2010) in order to reduce noise in distantly supervised corpora (Mintz et al., 2009). In DS, training instances are created from large, raw corpora using Knowledge Base (KB) entity linking and automatic annotation with heuristic rules. MIL in this setting considers multiple sentences (bags) that contain a pair of entities serving as the multiple instances of this pair. Verga et al. (2018) introduced another MIL setting for relation extraction between named entities in a document. In this setting, entities mapped to the same KB ID are considered as mentions of an entity concept and pairs of mentions correspond to the pair's multiple instances. However, document-level RE is not common in the general domain, as the entity types of interest can often be found in the same sentence (Banko et al., 2007). On the contrary, in the biomedical domain, document-level relations are particularly important given the numerous aliases that biomedical entities can have (Quirk and Poon, 2017).</p>
<p>To deal with document-level RE, recent approaches assume that only two mentions of the target entities reside in the document (Nguyen and Verspoor, 2018; Verga et al., 2018) or utilise different models for intra- and inter-sentence RE (Gu et al., 2016; Li et al., 2016b; Gu et al., 2017). In contrast with approaches that employ sequential models (Nguyen and Verspoor, 2018; Gu et al., 2017; Zhou et al., 2016), graph-based neural approaches have proven useful in encoding longdistance, inter-sentential information (Peng et al., 2017; Quirk and Poon, 2017; Gupta et al., 2019). These models interpret words as nodes and connections between them as edges. They typically perform on the nodes by updating the representations during training. However, a relation between two entities depends on different contexts. It could thus be better expressed with an edge connection that is unique for the pair. A straightforward way to address this is to create graph-based models that rely on edge representations rather focusing on node representations, which are shared between multiple entity pairs.</p> 
<p>In this work, we tackle document-level, intraand inter-sentence RE using MIL with a graphbased neural model. Our objective is to infer the relation between two entities by exploiting other interactions in the document. We construct a doc- ument graph with heterogeneous types of nodes and edges to better capture different dependencies between nodes. In the proposed graph, a node corresponds to either entities, mentions, or sentences, instead of words. We connect distinct nodes based on simple heuristic rules and generate different edge representations for the connected nodes. To achieve our objective, we design the model to be edge-oriented in a sense that it learns edge representations (between the graph nodes) rather than node representations. An iterative algorithm over the graph edges is used to model dependencies between the nodes in the form of edge representations. The intra- and inter-sentence entity relations are predicted by employing these edges. Our contributions can be summarised as follows:</p>
<ul>
<li>· We propose a novel edge-oriented graph neural model for document-level relation extraction. The model deviates from existing graph models as it focuses on constructing unique nodes and edges, encoding information into edge representations rather than node representations.</li>
<li>· The proposed model is independent of syntactic dependency tools and can achieve stateof-the-art performance on a manually annotated, document-level chemical-disease interaction dataset.</li>
<li>· Analysis of the model components indicates that the document-level graph can effectively encode document-level dependencies. Additionally, we show that inter-sentence associations can be beneficial for the detection of intrasentence relations.</li>
</ul>
<h2>2 Proposed Model</h2>
<p>We build our model as a significant extension of our previously proposed sentence-level model (Christopoulou et al., 2018) for documentlevel RE. The most critical difference between the two models is the introduction and construction of a partially-connected document graph, instead of a fully-connected sentence-level graph. Additionally, the document graph consists of heterogeneous types of nodes and edges in comparison with the sentence-level graph that contains only entity-nodes and single edge types among them. Furthermore, the proposed approach utilises multi-instance learning when mention-level annotations are available.</p>
<p>As illustrated in Figure 2, the proposed model consists of four layers: sentence encoding, graph</p>
<figure><figcaption><div class="caption">Figure 2: Abstract architecture of the proposed approach. The model receives a document and encodes each sentence separately. A document-level graph is constructed and fed into an iterative algorithm to generate edge representations between the target entity nodes. Some node connections are not shown for brevity.</div></figcaption></figure>
<p>construction, inference and classification layers. The model receives as input a document with identified concept-level entities and their textual mentions. Next, a document-level graph with multiple types of nodes and edges is constructed. An inference algorithm is applied on the graph edges to generate concept-level pair representations. In the final layer, the edge representations between the target concept-entity nodes are classified into relation categories.</p>
<p>For the remainder of this section, we first briefly introduce the document-level RE task setting and then explain the four layers of the proposed model.</p>
<h2>2.1 Task Setting</h2>
<p>In concept, document-level RE the input is considered an annotated document. The annotations include concept-level entities (with assigned KB IDs), as well as multiple occurrences of each entity under the same phrase of alias, i.e., entity mentions. We consider the associations of mentions to concept entities given (also known as entity linking (Shen et al., 2014)). The objective of the task is given an annotated document, to identify all the related concept-level pairs in that document. In this work, we refer to concept-level annotations as entities and mention-level annotations as mentions .</p>
<h2>2.2 Sentence Encoding Layer</h2>
<p>First, each word in the sentences of the input document is transformed into a dense vector representation, i.e., a word embedding. The vectorised words of each sentence are then fed into a Bidirectional LSTM network (BiLSTM) (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997), named the encoder. The output of the encoder results in contextualised representations for each word of the input sentence.</p>
<h2>2.3 Graph Layer</h2>
<p>The contextualised word representations from the encoder are used to construct a document-level graph structure. The graph layer comprises of two sub-layers, a node construction layer and an edge construction layer. We compose the representations of the graph nodes in the first sub-layer and the representations of the edges in the second.</p>
<h2>2.3.1 Node construction</h2>
<p>We form three distinct types of nodes in the graph: mention nodes (M) n m , entity nodes (E) n e , and sentence nodes (S) n s . Each node representation is computed as the average of the embeddings of different elements. Firstly, mention nodes correspond to different mentions of entities in the input document. The representation of a mention node is formed as the average of the words ( w ) that the mention contains. Secondly, entity nodes represent unique entity concepts. The representation of an entity node is computed as the average of the mention ( m ) representations associated with the entity. Finally, sentence nodes correspond to sentences. A sentence node is represented as the average of the word representations in the sentence. In order to distinguish different node types in the graph, we concatenate a node type ( ) embedding to each node represent tation. The final node representations are then estimated as n m = [avg w i ∈ m ( w i ); t m ] , n e = [avg m i ∈ e ( m i ); t e ] , n s = [avg w i ∈ s ( w i ); t s ] .</p>
<h2>2.3.2 Edge construction</h2>
<p>We initially construct non-directed edges between the graph nodes using heuristic rules that stem from the natural associations between the elements of a document, i.e., mentions, entities and sentences. As we cannot know in advance if two entities are related, we do not directly connect entity nodes. Connections between nodes are based on pre-defined document-level interactions. The model objective is to generate entity-to-entity (EE) edge representations using other existing edges in the graph and consequently infer entity-to-entity relations. The different pre-defined edge types are described below.</p>
<p>Mention-Mention (MM) : Co-occurrence of mentions in a sentence might be a weak indication of an interaction. For this reason, we create mentionto-mention edges only if the corresponding mentions reside in the same sentence. The edge representation between each mention pair m i and m j is generated by concatenating the representations of the nodes, the contexts c m ,m i j and a distance embedding associated with the distance between the two mentions d m ,m i j , in terms of intermediate words: x MM = [ n m i ; n m j ; c m ,m i j ; d m ,m i j ] . Here, we generate the context representation for these pairs in order to encode local, pair-centric information. We use an argument-based attention mechanism (Wang et al., 2016), to measure the importance of other words in the sentence towards the mention, denoting k ∈ { 1 2 , } as the mention arguments.</p>
<p>glyph[negationslash]</p>
<div class="formula-not-decoded">Formula not decoded</div>
<p>where n m k is a mention node representation, w i is a sentence word representation, a i is the attention weight of word i for mention pair m ,m 1 2 , H ∈ R w × d is a sentence word representations matrix, a ∈ R w is the attention weights vector for the pair and c m ,m 1 2 is the final context representation for the mention pair.</p>
<p>Mention-Sentence (MS) : Mention-to-sentence nodes are connected only if the mention resides in the sentence. Their initial edge representation is constructed as a concatenation of the mention and sentence nodes, x MS = [ n m ; n s ] .</p>
<p>Mention-Entity (ME) : We connect a mention node to an entity node if the mention is associated with the entity, x ME = [ n m ; n e ] .</p>
<p>Sentence-Sentence (SS) : Motivated by Quirk and Poon (2017), we connect sentence nodes to encode non-local information. The main differences with prior work is that our edges are unlabelled, non-directed and span multiple sentences. To encode the distance between sentences, we concatenate to the sentence node representations their distance in the form of an embedding: x SS = [ n s i ; n s j ; d s ,s i j ] . Weconnect all sentence nodes in the graph. We consider SSdirect as direct, ordered edges (distance equal to 1 ) and SSindirect as indirect, non-ordered edges (distance &gt; 1 ) between S nodes, respectively. In our setting, SS denotes the combination of SSdirect and SSindirect.</p>
<p>Entity-Sentence (ES) : To directly model entityto-sentence associations, we connect an entity node to a sentence node if at least one mention of the entity resides in this sentence, x ES = [ n e ; n s ] .</p>
<p>In order to result in edge representations of equal dimensionality, we use different linear reduction layers for different edge representations,</p>
<div class="formula-not-decoded">Formula not decoded</div>
<p>where e (1) z is an edge representation of length 1 , W z ∈ R d z × d corresponds to a learned matrix and z ∈ [ MM MS ME SS ES . , , , , ]</p>
<h2>2.4 Inference Layer</h2>
<p>We utilise an iterative algorithm to generate edges between different nodes in the graph, as well as to update existing edges. We initialise the graph only with the edges described in Section 2.3.2, meaning that direct entity-to-entity (EE) edges are absent. We can only generate EE edge representations by representing a path between their nodes. This implies that entities can be associated through an edge path of minimum length equal to 3 2 .</p>
<p>For this purpose, we adapt our two-step inference mechanism, proposed in Christopoulou et al. (2018), to encode interactions between nodes and edges in the graph and hence model EE associations.</p>
<p>At the first step, we aim to generate a path between two nodes i and j using intermediate nodes k . We thus combine the representations of two consecutive edges e ik and e kj , using a modified bilinear transformation. This action generates an edge representation of double length. We combine all existing paths between i and j through k . The i , j , and k nodes can be any of the three node types E, M, or S. Intermediate nodes without adjacent</p>
<p>2 Length 2 for an intra-sentence pair (E-S-E) or length 3 for an inter-sentence pair (E-S-S-E)</p>
<p>edges to the target nodes are ignored.</p>
<div class="formula-not-decoded">Formula not decoded</div>
<p>where σ is the sigmoid non-linear function, W ∈ R d z × d z is a learned parameter matrix, glyph[circledot] refers to element-wise multiplication, l is the length of the edge and e ik corresponds to the representation of the edge between nodes i and k .</p>
<p>During the second step, we aggregate the original (short) edge representation and the new (longer) edge representation resulted from Equation (3) with linear interpolation as follows:</p>
<p>glyph[negationslash]</p>
<div class="formula-not-decoded">Formula not decoded</div>
<p>where β ∈ [0 , 1] is a scalar that controls the contribution of the shorter edge presentation. In general β is larger for shorter edges as we expect that the relation between two nodes is better expressed through the shortest path between them (Xu et al., 2015; Borgwardt and Kriegel, 2005).</p>
<p>The two steps are repeated a finite number of times N . The number of iterations is correlated with the final length of the edge representations. With initial edge length l equal to 1 , the first iteration results in edges of length up-to 2 . The second iteration results in edges of length up-to 4 . Similarly, after N iterations, the length of edges will be up-to 2 N .</p>
<h2>2.5 Classification Layer</h2>
<p>To classify the concept-level entity pairs of interest, we incorporate a softmax classifier, using the entity-to-entity edges (EE) of the document graph that correspond to the concept-level entity pairs.</p>
<div class="formula-not-decoded">Formula not decoded</div>
<p>where W c ∈ R r × d z and b c ∈ R r are learned parameters of the classification layer and r is the number of relation categories.</p>
<h2>3 Experimental Settings</h2>
<p>The model was developed using PyTorch (Paszke et al., 2017). We incorporated early stopping to identify the best training epoch and used Adam (Kingma and Ba, 2015) as the model optimiser.</p>
<h2>3.1 Data and Task Settings</h2>
<p>We evaluated the proposed model on two datasets: CDR (BioCreative V): The Chemical-Disease Reactions dataset was created by Li et al. (2016a) for document-level RE. It consists of 1 500 , PubMed abstracts, which are split into three equally sized sets for training, development and testing. The dataset was manually annotated with binary interactions between Chemical and Disease concepts. For this dataset, we utilised PubMed pre-trained word embeddings (Chiu et al., 2016).</p>
<p>GDA (DisGeNet): The Gene-Disease Associations dataset was introduced by Wu et al. (2019), containing 30 192 , MEDLINE abstracts, split into 29 192 , articles for training and 1 000 , for testing. The dataset was annotated with binary interactions between Gene and Disease concepts at the document-level, using distant supervision. Associations between concepts were generated by aligning the DisGeNet (Pi˜ nero et al., 2016) platform with PubMed 3 abstracts. We further split the training set into a 80/20 percentage split as training and development sets. For the GDA dataset, we used randomly initialized word embeddings.</p>
<h2>3.2 Model Settings</h2>
<p>We explore multiple settings of the proposed graph using different edges (MM, ME, MS, ES, SS) and enhancements (node type embeddings, mention-pairs context embeddings, distance embeddings). We name our model EoG, an abbreviation of Edge-oriented Graph. We briefly describe the model settings in this section.</p>
<p>EoG refers to our main model with edges { MM, ME, MS, ES, SS } . The EoG ( Full ) setting refers to a model with a fully connected graph, where the graph nodes are all connected to each other, including E nodes. For this purpose, we introduce an additional linear layer for the EE edges as in Equation (2). The EoG ( NoInf ) setting refers to a no inference model, where the iterative inference algorithm (Section 2.4) is ignored. The concatenation of the entity node embeddings is used to represent the target pair. In this case, we also make use of an additional EE linear layer for EE edges. Finally, the EoG ( Sent ) setting refers to a model that was trained on sentences instead of documents. For each entity-level pair we merge the predictions of the mention-level pairs in different sentences using a maximum assumption: if at</p>
<p>3 https://www.ncbi.nlm.nih.gov/pubmed/</p>
<table><caption><div class="caption">Table 1: Overall, intra- and inter-sentence pairs performance comparison with the state-of-the-art on the CDR test set. The methods below the double line take advantage of additional training data and/or incorporate external tools.</div></caption><tbody><tr><th rowspan="2">Method</th><th colspan="3">Overall (%)</th><th colspan="3">Intra (%)</th><th colspan="3">Inter (%)</th></tr><tr><th>P</th><th>R</th><th>F1</th><th>P</th><th>R</th><th>F1</th><th>P</th><th>R</th><th>F1</th></tr><tr><th>Gu et al. (2017)</th><td>55.7</td><td>68.1</td><td>61.3</td><td>59.7</td><td>55.0</td><td>57.2</td><td>51.9</td><td>7.0</td><td>11.7</td></tr><tr><th>Verga et al. (2018)</th><td>55.6</td><td>70.8</td><td>62.1</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Nguyen and Verspoor (2018)</th><td>57.0</td><td>68.6</td><td>62.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>EoG</th><td>62.1</td><td>65.2</td><td>63.6</td><td>64.0</td><td>73.0</td><td>68.2</td><td>56.0</td><td>46.7</td><td>50.9</td></tr><tr><th>EoG ( Full )</th><td>59.1</td><td>56.2</td><td>57.6</td><td>71.2</td><td>62.3</td><td>66.5</td><td>37.1</td><td>42.0</td><td>39.4</td></tr><tr><th>EoG ( NoInf )</th><td>48.2</td><td>50.2</td><td>49.2</td><td>65.8</td><td>55.2</td><td>60.2</td><td>25.4</td><td>38.5</td><td>30.6</td></tr><tr><th>EoG ( Sent )</th><td>56.9</td><td>53.5</td><td>55.2</td><td>56.9</td><td>76.4</td><td>65.2</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Zhou et al. (2016)</th><td>55.6</td><td>68.4</td><td>61.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Peng et al. (2016)</th><td>62.1</td><td>64.2</td><td>63.1</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Li et al. (2016b)</th><td>60.8</td><td>76.4</td><td>67.7</td><td>67.3</td><td>52.4</td><td>58.9</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Panyam et al. (2018)</th><td>53.2</td><td>69.7</td><td>60.3</td><td>54.7</td><td>80.6</td><td>65.1</td><td>47.8</td><td>43.8</td><td>45.7</td></tr><tr><th>Zheng et al. (2018)</th><td>56.2</td><td>67.9</td><td>61.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table>
<p>least one mention-level prediction indicates a relation then we predict the entity pair as related, similarly to Gu et al. (2017). All of the settings incorporate node type embeddings, contextual embeddings for MM edges and distance embeddings for MM and SS edges, unless otherwise stated.</p>
<table><tbody><tr><th>Model</th><th colspan="6">Dev | Test F1 (%)</th></tr><tr><td></td><th colspan="2">Overall</th><th colspan="2">Intra</th><th colspan="2">Inter</th></tr><tr><th>EoG</th><td>78.7</td><td>81.5</td><td>82.5</td><td>85.2</td><td>48.8</td><td>50.0</td></tr><tr><th>EoG ( Full )</th><td>78.6</td><td>80.8</td><td>82.4</td><td>84.1</td><td>52.3</td><td>54.7</td></tr><tr><th>EoG ( NoInf )</th><td>71.8</td><td>74.6</td><td>76.8</td><td>79.1</td><td>45.5</td><td>49.3</td></tr><tr><th>EoG ( Sent )</th><td>73.8</td><td>73.8</td><td>78.1</td><td>78.8</td><td>-</td><td>-</td></tr></tbody></table>
<h2>4 Results</h2>
<p>Table 1 depicts the performance of our proposed model on the CDR test set, in comparison with the state-of-the-art. We directly compare our model with models that do not incorporate external knowledge. Verga et al. (2018) and Nguyen and Verspoor (2018) consider a single pair per document, while Gu et al. (2017) develops separate models for intra- and inter-sentence pairs. As it can be observed, the proposed model outperforms the state-of-the-art in CDR dataset by 1 3 . percentage points of overall performance. We also show the methods that take advantage of syntactic dependency tools. Li et al. (2016b) uses cotraining with additional unlabeled training data. Our model performs significantly better on intraand inter-sentential pairs, even compared to most of the models with external knowledge, except for Li et al. (2016b).</p>
<p>In addition, we report the performance of three baseline models. The EoG model outperforms all baselines for all pair types. In particular, for the inter-sentence pairs, performance significantly drops with a fully connected graph ( Full ) or without inference ( NoInf ). The former might indicate the existence of certain reasoning paths that should be followed in order to relate entities residing in different sentences. It is also important</p>
<p>Table 2: Performance comparison on the GDA development and test sets.</p>
<p>to note that the intra-sentence pairs substantially benefit from the document-level information, as EoG surpasses the performance of training on single sentences ( Sent ) by 3 %. Finally, the performance drop in intra-sentence pairs, as a result of the inference algorithm removal ( NoInf ), suggests that multiple entity associations exist in sentences (Christopoulou et al., 2018). Their interactions can be beneficial in cases of lack of word context information.</p>
<p>We also apply our model on the distantly supervised GDA dataset. As shown in Table 2 results for intra-sentence pairs are consistent with the findings of the CDR dataset for both development and test sets. This indicates that documentlevel information is helpful. However, performance differs for inter-sentence pairs and in particular for the fully connected graph ( Full ) baseline. We partially attribute this behavior to the small number of inter-sentence pairs in the GDA dataset (only 13 % compared to 30 % in the CDR dataset) that results in inadequate learning patters for EoG. We leave further investigation as part of future work.</p>
<table><caption><div class="caption">Table 3: Performance of EoG on the CDR test set with different pre-trained word embeddings.</div></caption><tbody><tr><th>Embeddings</th><th colspan="3">F1 (%)</th></tr><tr><td></td><th>Overall</th><th>Intra</th><th>Inter</th></tr><tr><th>EoG ( PubMed )</th><td>63.62</td><td>68.25</td><td>50.94</td></tr><tr><th>EoG ( GloVe )</th><td>63.01</td><td>67.52</td><td>50.26</td></tr><tr><th>EoG ( random )</th><td>61.41</td><td>66.80</td><td>46.51</td></tr></tbody></table>
<figure><figcaption><div class="caption">Figure 3: Performance as a function of the number of inference steps when using direct (SSdirect) or direct and indirect (SS) sentence-to-sentence edges, on the CDR development set.</div></figcaption></figure>
<h2>5 Analysis &amp; Discussion</h2>
<p>We first analyse the performance of our main model (EoG) using different pre-trained word embeddings. Table 3 shows the performance difference between domain-specific ( PubMed ) (Chiu et al., 2016), general-domain ( GloVe ) (Pennington et al., 2014) and randomly initialized ( random ) word embeddings. As observed, our proposed model performs consistently with both in-domain and out-of-domain pre-trained word embeddings. The low performance of random embeddings is due to the small size of the dataset, which results in lower quality embeddings.</p>
<p>For further analysis, we choose the CDR dataset as it is manually annotated. To better analyse the behaviour of our model, we conduct analysis on the effect of direct and indirect sentence-tosentence edges as a function of the inference steps. Figures 3a, 3b and 3c illustrate the performance of both graphs for overall, intra- and inter-sentence pairs respectively.</p>
<p>The first observation is that usage of direct edges only, reduces the overall performance al-</p>
<table><caption><div class="caption">Table 4: Ablation analysis for different edge and node types on the CDR development set.</div></caption><tbody><tr><td rowspan="2">Edge Types</td><th colspan="3">F1 (%)</th></tr><tr><th>Overall</th><th>Intra</th><th>Inter</th></tr><tr><th>EE</th><td>55.14</td><td>61.31</td><td>40.34</td></tr><tr><th>EoG</th><td>63.57</td><td>68.25</td><td>46.68</td></tr><tr><th>- MM</th><td>62.77</td><td>67.93</td><td>46.65</td></tr><tr><th>- ME</th><td>61.57</td><td>66.39</td><td>45.40</td></tr><tr><th>- MS</th><td>62.92</td><td>67.55</td><td>44.74</td></tr><tr><th>- ES</th><td>61.41</td><td>66.44</td><td>43.04</td></tr><tr><th>- SS indirect</th><td>59.70</td><td>67.09</td><td>28.00</td></tr><tr><th>- SS</th><td>57.41</td><td>65.45</td><td>1.59</td></tr><tr><th>- MM,ME,MS</th><td>60.46</td><td>66.07</td><td>39.56</td></tr><tr><th>- ES,MS,SS</th><td>56.86</td><td>64.63</td><td>0.00</td></tr></tbody></table>
<p>most by 4% , for inference step l = 8 . This drop mostly affects inter-sentence pairs, where a 18% point drop is observed. In fact, ordered edges (SSdirect) need longer inference to perform better, in comparison with additional indirect edges (SS) for which less steps are required. The superiority of SS edges, for all inference steps, compared to SSdirect edges on inter-sentence pairs detection, indicates that in a narrative, some intermediate information is not important. The observation that indirect edges perform slightly better than direct for intra-sentence pairs ( l ≤ 16 ) agrees with the results of Table 1 where we showed that intersentence information can act as complementary evidence for intra-sentence pairs.</p>
<p>We additionally conduct ablation analysis on the graph edges and nodes, as shown in Table 4. Usage of EE edges only results in poor performance across pairs. Removal of MM and ME edges does not significantly affect the performance as ES edges can replace their impact. Complete removal of connections to M nodes results in low inter-sentence performance. This behaviour pinpoints the importance of some local dependencies in identifying cross-sentence relations.</p>
<p>Removal of ES edges reduces the performance of all pairs, as encoding of EE edges becomes more difficult 4 . We further observe very poor identification of inter-sentence pairs without sentence-to-sentence connections. This is complementary with the inability of the model to identify any inter-sentence pairs without connections to S nodes. In this scenario, we enable identification of pairs across sentences only through MM and ME edges, as shown in Figure 4a. In the CDR dataset,</p>
<p>4 Length 3 (E-M-M-E) for intra- and length 5 (E-M-S-SM-E) for inter-sentence pairs.</p>
<p>(a) MM, ME edges</p>
<p>(b) ES, SS edges</p>
<table><caption><div class="caption">Figure 4: Relation paths with different types of edges.</div></caption><tbody><tr><td rowspan="2">Model</td><th colspan="3">F1 (%)</th></tr><tr><th>Overall</th><th>Intra</th><th>Inter</th></tr><tr><th>EoG</th><td>63.57</td><td>68.25</td><td>46.68</td></tr><tr><th>- node types</th><td>62.31</td><td>67.50</td><td>44.80</td></tr><tr><th>- MM context</th><td>62.88</td><td>67.67</td><td>46.59</td></tr><tr><th>- distances</th><td>62.53</td><td>68.00</td><td>41.53</td></tr><tr><th>- T,C,D</th><td>63.10</td><td>68.44</td><td>43.48</td></tr></tbody></table>
<p>Table 5: Ablation analysis of edge enhancements on the CDR development set.</p>
<p>78 % of inter-sentential pairs have at least one argument that is mentioned only once in the document. The identification of these pairs, without S nodes, requires very long inference paths . 5 As shown in Figure 4b, the introduction of S nodes results in a path with half the length, which we expect to better represent the relation. Longer inference representations are much weaker than shorter ones. This suggests that the inference mechanism has limited capability in identifying very complex associations.</p>
<p>We then investigate the additional enhancements of the graph edges in Table 5. In general, intra-sentence pairs are not affected by these settings. However, for inter-sentence pairs, removal of node type embeddings and distance embeddings results in a 2% and 5% drop in terms of F1score. These results indicate that the interactions between different elements in a document, along with the distance between sentences and mentions, play an important role in inter-sentence pair inference. Removing all of these settings does not perform worse than removing one of them, which might indicate model overfitting. We plan to further investigate this as part of future work.</p>
<p>We examine the performance of different models on inter-sentence pairs, based on their sentence-level distances. Figure 5 illustrates that for long-distanced pairs, EoG has lower performance, indicating the difficulty in predicting them and a possible requirement for other, latent document-level information (EoG ( Full )).</p>
<p>5 Minimum inference length 6 (E-M-M-E-M-M-E).</p>
<figure><figcaption><div class="caption">Figure 5: Performance of inter-sentence pairs on the CDR development set as a function of their sentence distance.</div></figcaption></figure>
<p>Following short exposure to oral prednisone [...]. Both presented in the emergency room with profound coma hypotension , , severe hyperglycemia , and acidosis.</p>
<p>The etiology of pyeloureteritis cystica has long been [...] The disease occurred subsequent to the initiation of heparin therapy [...]</p>
<p>Time trends in warfarin -associated hemorrhage . [...] The proportion of patients with major and intracranial bleeding increased [...]</p>
<p>Table 6: Inter-sentence pairs from the CDR development set that EoG fails to detect.</p>
<p>As final analysis, we investigate some of the cases where the graph models are unable to identify inter-sentence related pairs. For this purpose, we randomly check some of the common false negative errors among the EoG models. We identify three frequent cases of errors, as shown in Table 6. In the first case, when multiple entities reside in the same sentence and are connected with conjunctions (e.g., 'and') or commas, the model often failed to find associations with all of them. The second error derives from missing coreference connections. For instance, pyeloureteritis cystica is referred to as disease . Although our model cannot directly create these edges, S nodes potentially simulate such links, by encoding the co-referring entities into the sentence representation. Finally, incomplete entity linking results into additional model errors. For instance, in the third example, hemorrhage and intracranial bleeding are synonymous terms. However, they are assigned different KB IDs, hence treated as different entities. The model can find the intra-sentential relation but not the inter-sentential one.</p>
<h2>6 Related Work</h2>
<p>Traditional approaches focus on intra-sentence supervised RE, utilising CNN or RNN, ignoring multiple entities in a sentence (Zeng et al., 2014; Nguyen and Grishman, 2015) as well as incorporating external syntactic tools (Miwa and Bansal,</p>
<p>2016; Zhang et al., 2018). Christopoulou et al. (2018) considered intra-sentence entity interactions without domain dependencies by modelling long dependencies between the entities of a sentence.</p>
<p>Other approaches deal with distantlysupervised datasets but are also limited to intra-sentential relations. They utilise Piecewise Convolutional Neural Networks (PCNN) (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Zhou et al., 2018), entity descriptors (Jiang et al., 2016) and graph CNNs (Vashishth et al., 2018) to perform MIL on bags-of-sentences that contain multiple mentions of an entity pair. Recently, Zeng et al. (2017) proposed a method for extracting paths between entities using the target entities' mentions in several different sentences (in possibly different documents) as intermediate connectors. They allow mention-mention edges only if these mentions belong to the same entity and consider that a single mention pair exists in a sentence. On the contrary, we not only allow interactions between all mentions in the same sentence, but also consider multiple edges between mentions, entities and sentences in a document.</p>
<p>Current approaches that try to deal with document-level RE are mostly graph-based. Quirk and Poon (2017) introduced the notion of a document graph, where nodes are words and edges represent intraand inter-sentential relations between the words. They connected words with different dependency edges and trained a binary logistic regression classifier. They evaluated their model on distantly supervised full-text articles from PubMed for Gene-Drug associations, restricting pairs within a window of consecutive sentences. Following this work, other approaches incorporated graphical models for document-level RE such as graph LSTM (Peng et al., 2017), graph CNN (Song et al., 2018) or RNNs on dependency tree structures (Gupta et al., 2019). Recently, Jia et al. (2019) improved n -ary RE using information from multiple sentences and paragraphs in a document. Similar to our approach, they choose to directly classify concept-level pairs rather than multiple mention-level pairs. Although they consider sub-relations to model related tuples, they ignore interactions with other entities outside of the target tuple in the discourse units.</p>
<p>Non-graph-based approaches utilise different intraand inter-sentence models and merge the resulted predictions (Gu et al., 2016, 2017). Other approaches extract document-level representations for each candidate entity pair (Zheng et al., 2018; Li et al., 2018; Wu et al., 2019), or use syntactic dependency structures (Zhou et al., 2016; Peng et al., 2016). Verga et al. (2018) proposed a Transformer-based model for documentlevel relation extraction with multi-instance learning, merging multiple mention pairs. Nguyen and Verspoor (2018) used a CNN with additional character-level embeddings. Singh and Bhatia (2019) also utilised Transformer and connected two target entities by combining them directly and via a contextual token. However, they consider a single target entity pair per document.</p>
<h2>7 Conclusion</h2>
<p>We presented a novel edge-oriented graph neural model for document-level relation extraction using multi-instance learning. The proposed model constructs a document-level graph with heterogeneous types of nodes and edges, modelling intraand inter-sentence pairs simultaneously with an iterative algorithm over the graph edges. To the best of our knowledge, this is the first approach to utilise an edge-oriented model for document-level RE.</p>
<p>Analysis on intra- and inter-sentence pairs indicated that the proposed, partially-connected, document graph structure can effectively encode dependencies between document elements. Additionally, we deduce that document-level information can contribute to the identification of intrasentence pairs leading to higher precision and F1score.</p>
<p>As future work, we plan to improve the inference mechanism and potentially incorporate additional information in the document-graph structure. We hope that this study will inspire the community to further investigate the usage of edgeoriented models on RE and other related tasks.</p>
<h2>Acknowledgments</h2>
<p>The authors would like to thank the anonymous reviewers for their comments and AIST/AIRC for providing the computational resources for this study. Research was funded by the University of Manchester James Elson Studentship Award and the BBSRC Japan Partnering Award BB/P025684/1.</p>
<h2>References</h2>
<p>Michele Banko, Michael J Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Proceedings of the International Joint Conference on Artifical Intelligence , pages 2670-2676. Morgan Kaufmann Publishers Inc.</p>  
<p>Karsten M Borgwardt and Hans-Peter Kriegel. 2005. Shortest-path kernels on graphs. In Proceedings of the IEEE International Conference on Data Mining , pages 74-81. IEEE Computer Society.</p>
<p>Billy Chiu, Gamal Crichton, Anna Korhonen, and Sampo Pyysalo. 2016. How to train good word embeddings for biomedical nlp. In Proceedings of the BioNLP workshop , pages 166-174.</p>
<p>Fenia Christopoulou, Makoto Miwa, and Sophia Ananiadou. 2018. A walk-based model on entity graphs for relation extraction. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (Volume 2) , pages 81-88. Association for Computational Linguistics.</p>
<p>Jinghang Gu, Longhua Qian, and Guodong Zhou. 2016. Chemical-induced disease relation extraction with various linguistic features. Database .</p>
<p>Jinghang Gu, Fuqing Sun, Longhua Qian, and Guodong Zhou. 2017. Chemical-induced disease relation extraction via convolutional neural network. Database .</p>
<p>Pankaj Gupta, Subburam Rajaram, Hinrich Schtze, and Thomas Runkler. 2019. Neural relation extraction within and across sentence boundaries. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 33, pages 6513-6520.</p>
<p>Sepp Hochreiter and J¨rgen u Schmidhuber. 1997. Long short-term memory. Neural computation , 9(8):1735-1780.</p>
<p>Robin Jia, Cliff Wong, and Hoifung Poon. 2019. Document-level n-ary relation extraction with multiscale representation learning. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1) , pages 36933704. Association for Computational Linguistics.</p>
<p>Xiaotian Jiang, Quan Wang, Peng Li, and Bin Wang. 2016. Relation extraction with multi-instance multilabel convolutional neural networks. In Proceedings of the International Conference on Computational Linguistics: Technical Papers , pages 14711480. The COLING Organizing Committee.</p>
<p>Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations .</p>
<p>Haodi Li, Ming Yang, Qingcai Chen, Buzhou Tang, Xiaolong Wang, and Jun Yan. 2018. Chemicalinduced disease extraction via recurrent piecewise convolutional neural networks. BMC medical informatics and decision making , 18(2):60.</p>
<p>Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Thomas C Wiegers, and Zhiyong Lu. 2016a. Biocreative v cdr task corpus: a resource for chemical disease relation extraction. Database .</p>       
<p>Zhiheng Li, Zhihao Yang, Hongfei Lin, Jian Wang, Yingyi Gui, Yin Zhang, and Lei Wang. 2016b. CIDextractor: A chemical-induced disease relation extraction system for biomedical literature. In IEEE International Conference on Bioinformatics and Biomedicine , pages 994-1001. IEEE.</p>
<p>Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. 2016. Neural relation extraction with selective attention over instances. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (Volume 1) , pages 21242133.</p>
<p>Yi Luan, Dave Wadden, Luheng He, Amy Shah, Mari Ostendorf, and Hannaneh Hajishirzi. 2019. A general framework for information extraction using dynamic span graphs. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1) , pages 3036-3046. Association for Computational Linguistics.</p>
<p>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (Volume 2) , pages 1003-1011. Association for Computational Linguistics.</p>
<p>Makoto Miwa and Mohit Bansal. 2016. End-to-end relation extraction using LSTMs on sequences and tree structures. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (Volume 1) , pages 1105-1116. Association for Computational Linguistics.</p>
<p>Dat Quoc Nguyen and Karin Verspoor. 2018. Convolutional neural networks for chemical-disease relation extraction are improved with character-based word embeddings. In Proceedings of the BioNLP workshop , pages 129-136. Association for Computational Linguistics.</p>       
<p>Thien Huu Nguyen and Ralph Grishman. 2015. Relation extraction: Perspective from convolutional neural networks. In Proceedings of the Workshop on Vector Space Modeling for Natural Language Processing , pages 39-48.</p>
<p>Nagesh C Panyam, Karin Verspoor, Trevor Cohn, and Kotagiri Ramamohanarao. 2018. Exploiting graph kernels for high performance biomedical relation extraction. Journal of biomedical semantics , 9(1):7.</p>
<p>Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in pytorch. In NIPS-W .</p>
<p>Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen-tau Yih. 2017. Cross-sentence n-ary relation extraction with graph lstms. Transactions of the Association for Computational Linguistics , 5:101-115.</p>
<p>Yifan Peng, Chih-Hsuan Wei, and Zhiyong Lu. 2016. Improving chemical disease relation extraction with rich features and weakly labeled data. Journal of cheminformatics , 8(1):53.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the Empirical Methods in Natural Language Processing , pages 1532-1543. Association for Computational Linguistics.</p>
<p>Janet Pi˜ nero, ` lex A Bravo, N´ uria Queralt-Rosinach, Alba Guti´ errez-Sacrist´n, a Jordi Deu-Pons, Emilio Centeno, Javier Garc´ ıa-Garc´ ıa, Ferran Sanz, and Laura I Furlong. 2016. Disgenet: a comprehensive platform integrating information on human diseaseassociated genes and variants. Nucleic acids research , pages D833-D839.</p>
<p>Chris Quirk and Hoifung Poon. 2017. Distant supervision for relation extraction beyond the sentence boundary. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (Volume 1) , pages 1171-1182. Association for Computational Linguistics.</p>
<p>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases , pages 148-163. Springer.</p>
<p>Mike Schuster and Kuldip K Paliwal. 1997. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing , 45(11):2673-2681.</p>
<p>Wei Shen, Jianyong Wang, and Jiawei Han. 2014. Entity linking with a knowledge base: Issues, techniques, and solutions. IEEE Transactions on Knowledge and Data Engineering , 27(2):443-460.</p>
<p>Gaurav Singh and Parminder Bhatia. 2019. Relation extraction using explicit context conditioning. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 1442-1447. Association for Computational Linguistics.</p>
<p>Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. N-ary relation extraction using graphstate lstm. In Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 2226-2235. Association for Computational Linguistics.</p>
<p>Shikhar Vashishth, Rishabh Joshi, Sai Suman Prayaga, Chiranjib Bhattacharyya, and Partha Talukdar. 2018. Reside: Improving distantly-supervised neural relation extraction using side information. In Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 1257-1266. Association for Computational Linguistics.</p>
<p>Patrick Verga, Emma Strubell, and Andrew McCallum. 2018. Simultaneously self-attending to all mentions for full-abstract biological relation extraction. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1) , pages 872-884. Association for Computational Linguistics.</p>
<p>Linlin Wang, Zhu Cao, Gerard de Melo, and Zhiyuan Liu. 2016. Relation classification via multi-level attention CNNs. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (Volume 1) , pages 1298-1307. Association for Computational Linguistics.</p>
<p>Ye Wu, Ruibang Luo, Henry CM Leung, Hing-Fung Ting, and Tak-Wah Lam. 2019. Renet: A deep learning approach for extracting gene-disease associations from literature. In International Conference on Research in Computational Molecular Biology , pages 272-284. Springer.</p>  
<p>Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng, and Zhi Jin. 2015. Classifying relations via long short term memory networks along shortest dependency paths. In Proceedings of Conference on Empirical Methods in Natural Language Processing , pages 1785-1794. Association for Computational Linguistics.</p>
<p>Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. Distant supervision for relation extraction via piecewise convolutional neural networks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 1753-1762.</p>
<p>Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation classification via convolutional deep neural network. In Proceedings of the International Conference on Computational Linguistics: Technical Papers , pages 2335-2344. Dublin City University and Association for Computational Linguistics.</p>
<p>Wenyuan Zeng, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2017. Incorporating relation paths in neural relation extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 1768-1777. Association for Computational Linguistics.</p>
<p>Yuhao Zhang, Peng Qi, and Christopher D Manning. 2018. Graph convolution over pruned dependency trees improves relation extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 2205-2215. Association for Computational Linguistics.</p>
<p>Wei Zheng, Hongfei Lin, Zhiheng Li, Xiaoxia Liu, Zhengguang Li, Bo Xu, Yijia Zhang, Zhihao Yang, and Jian Wang. 2018. An effective neural model extracting document level chemical-induced disease relations from biomedical literature. Journal of biomedical informatics , 83:1-9.</p>
<p>Huiwei Zhou, Huijie Deng, Long Chen, Yunlong Yang, Chen Jia, and Degen Huang. 2016. Exploiting syntactic and semantics information for chemicaldisease relation extraction. Database .</p>
<p>Peng Zhou, Jiaming Xu, Zhenyu Qi, Hongyun Bao, Zhineng Chen, and Bo Xu. 2018. Distant supervision for relation extraction with hierarchical selective attention. Neural Networks , 108:240-247.</p>
<h2>A Datasets</h2>
<p>In Tables 7-8 we summarise the statistics for the CDR and GDA datasets, respectively. For all datasets, we used the GENIA Sentence Splitter 6 and GENIA Tagger 7 for sentence splitting and word tokenisation respectively. We additionally removed mentions in the given abstracts that were not grounded to a Knowledge Base ID (ID equal to -1 ).</p>
<p>Due to the small size of the CDR dataset, some approaches create a new split from the union of train and development sets (Verga et al., 2018; Zhou et al., 2018). We select to merge the train and development sets and re-train our model on their entire union for evaluation on the test set following Lin et al. (2016) and Zhou et al. (2016). To compare with related work, we followed Verga et al. (2018) and Gu et al. (2016) and ignored nonrelated pairs that correspond to general concepts (MeSH vocabulary hypernym filtering).</p>
<h2>B Hyper-parameter Setting</h2>
<p>We used the development set to identify the stopping training epoch and tune the number of inference iterations. Except from these parameters, all experiments used the same hyperparameters, with a fixed initialisation seed. For the CDR dataset EoG, ( Full ) and ( Sent ) models performed best with l = 8 2 4 , , inference steps, respectively. The</p>
<p>http://www.nactem.ac.uk/y-matsu/</p>
<p>6 geniass/</p>
<p>7 http://www.nactem.ac.uk/GENIA/tagger/</p>
<table><tbody><tr><td></td><th>Train</th><th>Dev</th><th>Test</th></tr><tr><th>Documents</th><td>500</td><td>500</td><td>500</td></tr><tr><th>Positive pairs</th><td>1,038</td><td>1,012</td><td>1,066</td></tr><tr><th>Intra</th><td>754</td><td>766</td><td>747</td></tr><tr><th>Inter</th><td>284</td><td>246</td><td>319</td></tr><tr><th>Negative pairs</th><td>4,202</td><td>4,075</td><td>4,138</td></tr><tr><th>Entities</th><td></td><td></td><td></td></tr><tr><th>Chemical</th><td>1,467</td><td>1,507</td><td>1,434</td></tr><tr><th>Disease</th><td>1,965</td><td>1,864</td><td>1,988</td></tr><tr><th>Mentions</th><td></td><td></td><td></td></tr><tr><th>Chemical</th><td>5,162</td><td>5,307</td><td>5,370</td></tr><tr><th>Disease</th><td>4,252</td><td>4,328</td><td>4,430</td></tr></tbody></table>
<p>Table 7: CDR (BioCreative V) dataset statistics.</p>
<table><tbody><tr><td></td><th>Train</th><th>Dev</th><th>Test</th></tr><tr><th>Documents</th><td>23,353</td><td>5,839</td><td>1,000</td></tr><tr><th>Positive pairs</th><td>36,079</td><td>8,762</td><td>1,502</td></tr><tr><th>Intra</th><td>30,905</td><td>7,558</td><td>1,305</td></tr><tr><th>Inter</th><td>5,174</td><td>1,204</td><td>197</td></tr><tr><th>Negative pairs</th><td>96,399</td><td>24,362</td><td>3,720</td></tr><tr><th>Entities</th><td></td><td></td><td></td></tr><tr><th>Gene</th><td>46,151</td><td>11,406</td><td>1,903</td></tr><tr><th>Disease</th><td>67,257</td><td>16,703</td><td>2,778</td></tr><tr><th>Mentions</th><td></td><td></td><td></td></tr><tr><th>Gene</th><td>205,457</td><td>51,410</td><td>8,404</td></tr><tr><th>Disease</th><td>226,015</td><td>56,318</td><td>9,524</td></tr></tbody></table>
<p>Table 8: GDA (DisGeNet) dataset statistics.</p>
<p>chosen batchsize was equal to 2 . For the GDA dataset, EoG and EoG ( Full ) performed best with l = 16 and EoG ( Sent ) with l = 8 inference steps. The chosen batchsize was equal to 3 . For all experiments performance was measured in terms of micro precision (P), recall (R) and F1score (F1). We list the hyper-parameters used to train the proposed model in Table 9.</p>
<table><caption><div class="caption">Table 9: Hyper-parameter values used in the reported experiments.</div></caption><tbody><tr><td>Parameter</td><th>Value</th></tr><tr><th>Batch size</th><td>[2 , 3]</td></tr><tr><th>Learning rate</th><td>0 . 002</td></tr><tr><th>Gradient clipping</th><td>10</td></tr><tr><th>Early stop patience</th><td>10</td></tr><tr><th>Regularization</th><td>10 - 4</td></tr><tr><th>Dropout word embedding layer</th><td>0 . 5</td></tr><tr><th>Dropout classification layer</th><td>0 . 3</td></tr><tr><th>Word dimension</th><td>200</td></tr><tr><th>Node type dimension</th><td>10</td></tr><tr><th>Distance dimension</th><td>10</td></tr><tr><th>Edge dimension</th><td>100</td></tr><tr><th>β</th><td>0 . 8</td></tr><tr><th>Optimizer</th><td>Adam</td></tr><tr><th>Inference iterations</th><td>[0 , 5]</td></tr></tbody></table>
</div>
</body>
</html>